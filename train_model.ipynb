{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorchvideo transformers evaluate -q\n",
    "# !pip install accelerate -U\n",
    "# !pip install torchvision==0.16.0\n",
    "# !pip install pytorchvideo\n",
    "# !pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Define the root path\n",
    "dataset_root_path = 'data-split'\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "\n",
    "# Get the folder names (surf classes)\n",
    "surf_classes = [item.name for item in dataset_root_path.glob(\"*\") if item.is_dir()]\n",
    "\n",
    "# List to store all mp4 files\n",
    "full_dataset = []\n",
    "\n",
    "# Iterate over surf classes and aggregate mp4 files\n",
    "for surf_class in surf_classes:\n",
    "    print(surf_class)\n",
    "    surf_class_path = dataset_root_path / surf_class\n",
    "    mp4_files = surf_class_path.glob(\"**/*.mp4\")\n",
    "    full_dataset.extend(mp4_files)\n",
    "    # print('\\n'*3)\n",
    "\n",
    "# Convert to list for easy access if needed\n",
    "full_dataset = list(full_dataset)\n",
    "\n",
    "display(full_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "def split_data(dataset, random_seed=42):\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    test_size = int(0.2 * len(dataset))\n",
    "    val_size = len(dataset) - train_size - test_size\n",
    "\n",
    "    train_dataset, temp_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "    val_dataset, test_dataset = torch.utils.data.random_split(temp_dataset, [val_size, test_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Define the root path\n",
    "dataset_root_path = 'data-split'\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "\n",
    "# Iterate over surf classes\n",
    "for surf_class in surf_classes:\n",
    "    if surf_class not in ['train', 'val', 'test']:\n",
    "        surf_class_path = dataset_root_path / surf_class\n",
    "        mp4_files = list(surf_class_path.glob(\"**/*.mp4\"))\n",
    "\n",
    "        # Create directories for train, val, and test\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            os.makedirs(os.path.join('data-split', split, surf_class), exist_ok=True)\n",
    "\n",
    "        # Call the function to split the data\n",
    "        train_dataset, val_dataset, test_dataset = split_data(mp4_files)\n",
    "\n",
    "        # Move files to respective directories\n",
    "        for dataset, split in zip([train_dataset, val_dataset, test_dataset], ['train', 'val', 'test']):\n",
    "            for file_path in dataset:\n",
    "                shutil.move(file_path, os.path.join('data-split', split, surf_class, os.path.basename(file_path)))\n",
    "\n",
    "for item in dataset_root_path.iterdir():\n",
    "    if item.is_dir() and item.name not in ['train', 'val', 'test']:\n",
    "        shutil.rmtree(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_path = 'data-split'\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"val/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/*.mp4\"))\n",
    ")\n",
    "class_labels = sorted({str(path).split(\"\\\\\")[2] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, Resize\n",
    "from pytorchvideo.transforms import UniformTemporalSubsample, Normalize, RandomShortSideScale, ApplyTransformToKey\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/*.mp4\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/*.mp4\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/*.mp4\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification,VideoMAEConfig\n",
    "\n",
    "# Create a new configuration with the desired image size\n",
    "# config = VideoMAEConfig(\n",
    "#     # image_size=(640, 360)\n",
    "#     image_size=(360, 360)\n",
    "# )\n",
    "\n",
    "# Now, use this configuration when initializing your model\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"  # pre-trained model from which to fine-tune\n",
    "batch_size = 4  # batch size for training and evaluation\n",
    "\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training Configuration\n",
    "args = TrainingArguments(\n",
    "    \"fine-tuned-model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    # push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Evaluation Metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions.argmax(axis=1)\n",
    "    references = eval_pred.label_ids\n",
    "    return metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Define Collate Function\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"video\"].permute(1, 0, 2, 3) for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 16\n",
    "# sample_rate = 32\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "\n",
    "# Training dataset transfor mations.\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    # RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    # RandomCrop(resize_to),\n",
    "                    # RandomHorizontalFlip(p=0.5),\n",
    "                    Resize((224, 224)),  # Resize images to 224x224\n",
    "\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Training dataset.\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# Validation and evaluation datasets' transformations.\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    # Resize(resize_to),\n",
    "                    Resize((224, 224)),  # Resize images to 224x224\n",
    "\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation and evaluation datasets.\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a counter\n",
    "# dataset_size = 0\n",
    "\n",
    "# # Iterate through the dataset and count the number of samples\n",
    "# for _ in train_dataset:\n",
    "#     dataset_size += 1\n",
    "\n",
    "# print(\"Training dataset size:\", dataset_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = train_dataset.num_videos + val_dataset.num_videos + test_dataset.num_videos\n",
    "train = train_dataset.num_videos/tot*100\n",
    "val =val_dataset.num_videos/tot*100\n",
    "test =test_dataset.num_videos/tot*100\n",
    "\n",
    "print(f'tot:{tot}')\n",
    "print(f'train: {train:.2f}%\\n')\n",
    "print(f'test: {test:.2f}%\\n')\n",
    "print(f'val: {val:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an iterator for the dataset\n",
    "# train_iterator = iter(train_dataset)\n",
    "# for _ in range(9):\n",
    "#     next(train_iterator)\n",
    "# tenth_video = next(train_iterator)\n",
    "# tenth_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video = next(iter(train_dataset))\n",
    "# print(sample_video)\n",
    "sample_video.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_video(sample_video):\n",
    "    \"\"\"Utility to investigate the keys present in a single video sample.\"\"\"\n",
    "    for k in sample_video:\n",
    "        # print(k)\n",
    "        if k == \"video\":\n",
    "            # print(k, sample_video[\"video\"].shape)\n",
    "            continue\n",
    "        else:\n",
    "            # print(k, sample_video[k])\n",
    "            continue\n",
    "\n",
    "    print(f\"Video label: {id2label[sample_video[k]]}\\n\")\n",
    "\n",
    "\n",
    "# investigate_video(sample_video)\n",
    "# train_iterator = iter(train_dataset)\n",
    "# sample_videos = [next(train_iterator) for _ in range(3)]\n",
    "# for sample_video in sample_videos:\n",
    "#     # investigate_video(sample_video)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 4}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_video_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video.keys()\n",
    "sample_video['video_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensor = sample_video\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt.split(\"/\")[-1]\n",
    "model_ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-2\"\n",
    "num_epochs = 3\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions.\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"The collation function to be used by `Trainer` to prepare data batches.\"\"\"\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "print(trainer.evaluate(train_dataset))\n",
    "print(trainer.evaluate(test_dataset))\n",
    "print(trainer.evaluate(val_dataset))\n",
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = train_results.training_loss\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "val_loss = val_results['eval_loss']\n",
    "loss_ratio = val_loss / train_loss\n",
    "print(\"Validation Loss / Training Loss Ratio:\", loss_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "trainer.log_metrics(\"test\", test_results)\n",
    "trainer.save_metrics(\"test\", test_results)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = VideoMAEForVideoClassification.from_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, video):\n",
    "    \"\"\"Utility to run inference given a model and test video.\n",
    "\n",
    "    The video is assumed to be preprocessed already.\n",
    "    \"\"\"\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    perumuted_sample_test_video = video[\"video\"].permute(1, 0, 2, 3)\n",
    "\n",
    "    inputs = {\n",
    "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
    "        \"labels\": torch.tensor(\n",
    "            [video[\"label\"]]\n",
    "        ),  # this can be skipped if you don't have labels available.\n",
    "    }\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_video = next(iter(test_dataset))\n",
    "investigate_video(sample_test_video)\n",
    "logits = run_inference(trained_model, sample_test_video)\n",
    "display_gif(sample_test_video[\"video\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits)\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
